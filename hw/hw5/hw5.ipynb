{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# initializing otter-grader\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0f9b2de18190d9d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 5: Predicting Housing Prices\n",
    "\n",
    "## Due Date: 11:59pm Friday, May 29.**\n",
    "\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the homework, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the collaborators cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *list names here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62cfd21463535cac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f68729731e7fe39d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# The Data\n",
    "\n",
    "The [Ames dataset](http://jse.amstat.org/v19n3/decock.pdf) consists of 2930 records taken from the Ames, Iowa, Assessorâ€™s Office describing houses sold in Ames from 2006 to 2010.  The data set has 23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables (and 2 additional observation identifiers) --- 82 features in total.  An explanation of each variable can be found in the included `codebook.txt` file.  The information was used in computing assessed values for individual residential properties sold in Ames, Iowa from 2006 to 2010.  **Some noise has been added to the actual sale price, so prices will not match official records.**\n",
    "\n",
    "The data are split into training and test sets with 2000 and 930 observations, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8fea30adc9d489b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"ames_train.csv\")\n",
    "test_data = pd.read_csv(\"ames_test.csv\")\n",
    "\n",
    "print(\"Training\", training_data.shape)\n",
    "print(\"Testing\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d6d509b6e854e10",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As a good sanity check, we should verify that the data shape matches the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c841a2de55691502",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# 2000 observations and 82 features in training data\n",
    "assert training_data.shape == (2000, 82)\n",
    "# 930 observations and 81 features in test data\n",
    "assert test_data.shape == (930, 81)\n",
    "# SalePrice is hidden in the test data\n",
    "assert 'SalePrice' not in test_data.columns.values\n",
    "# Every other column in the test data should be in the training data\n",
    "assert len(np.intersect1d(test_data.columns.values, \n",
    "                          training_data.columns.values)) == 81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce9acc2f62c96e59",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The next order of business is getting a feel for the variables in our data.  The Ames data set contains information that typical homebuyers would want to know.  A more detailed description of each variable is included in [`codebook.txt`](./codebook.txt), which is included in [the same folder](./) as the notebook.  **You should take some time to familiarize yourself with the codebook before moving forward.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e60a7a0cda5eecf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "training_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you cannot see the values for all columns. In order to tell Jupyter notebook to display values for all columns, we can remove the constraint on the number of columns that are being displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba0f6926b0dafefb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 1: Exploratory Data Analysis\n",
    "\n",
    "In this section, we will make a series of exploratory visualizations and interpret them.\n",
    "\n",
    "Note that we will perform EDA on the **training data** so that information from the test data does not influence our modeling decisions. Usually, you should always work with your **training data**, leaving **testing data** until the very end, when you want to check the accuracy of the models that you trained.\n",
    "\n",
    "### Sale Price\n",
    "We begin by examining a density plot of our target variable `SalePrice`.  At the same time, we can also take a look at some descriptive statistics of this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15d483a695655cea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "training_data\n",
    "\n",
    "alt.Chart(training_data).transform_density(\n",
    "    'SalePrice',\n",
    "    as_=['SalePrice', 'density'],\n",
    ").mark_area().encode(\n",
    "    x=\"SalePrice:Q\",\n",
    "    y='density:Q',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45e5037c06db70f0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how far apart lie the `min` and `max` values, especially compared to the `mean` and the median (the `50%` value). **Reminder**: the median value is the \"middle\" of a sorted list of numbers [1](https://www.mathsisfun.com/median.html), which means that 50\\% of the values in the list lie below and above this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-592d5f41ebd67ee2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 1: `SalePrice` vs. living area  <a name=\"q1\"></a>\n",
    "To check your understanding of the graph and summary statistics above, answer the following `True` or `False` questions:\n",
    "\n",
    "1. The distribution of `SalePrice` in the training set is left-skew.\n",
    "1. The mean of `SalePrice` in the training set is greater than the median.\n",
    "1. At least 25% of the houses in the training set sold for more than \\$200,000.00.\n",
    "\n",
    "*The provided tests for this question do not confirm that you have answered correctly; only that you have assigned each variable to `True` or `False`.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1-answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# These should be True or False\n",
    "q1statement1 = ...\n",
    "q1statement2 = ...\n",
    "q1statement3 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9e22aac9b45f88e3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### SalePrice vs Gr_Liv_Area\n",
    "\n",
    "Next, visualize the association between `SalePrice` and `Gr_Liv_Area`.  The [`codebook.txt`](./codebook.txt) file tells us that `Gr_Liv_Area` measures \"above grade (ground) living area square feet.\"\n",
    "\n",
    "This variable represents the square footage of the house excluding anything underground.  Some additional research (into real estate conventions) reveals that this value also excludes the garage space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-02a467f8950ee680",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "chart = alt.Chart(training_data).mark_point().encode(\n",
    "    x='Gr_Liv_Area',\n",
    "    y='SalePrice'\n",
    ")\n",
    "\n",
    "chart + chart.transform_regression('Gr_Liv_Area', 'SalePrice').mark_line(color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e69fbfdd6101f836",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "There's certainly an association, and perhaps it's linear, but the spread is wider at larger values of both variables.  Also, there are two particularly suspicious houses above 5000 square feet that look too inexpensive for their size.  Altair automatically fits the simple linear regression based on this one predictor variable, `Gr_Liv_Area`.  Later, we'll add additional predictors to our model and manually fit the linear regression model using `sklearn` library.\n",
    "\n",
    "## Question 2: identifying outliers <a name=\"q2\"></a>\n",
    "What are the Parcel Indentification Numbers for the two houses with `Gr_Liv_Area` greater than 5000 sqft?\n",
    "\n",
    "*Hint*: You can answer this question in one line using [`training_data.query`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html) and extracting the `PID` column.\n",
    "\n",
    "*The provided tests for this question do not confirm that you have answered correctly; only that you have assigned `q2house1` and `q2house2` to two integers that are in the range of PID values.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb0c9f329767dfc2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Hint: You can answer this question in one line\n",
    "# (q2house1, q2house2) = ...\n",
    "...\n",
    "print(q2house1, q2house2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf7fe5dcd37df6f9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 3: processing outliers <a name=\"q3\"></a>\n",
    "\n",
    "The codebook tells us how to manually inspect the houses using an online database called Beacon. These two houses are true outliers in this data set: they aren't the same time of entity as the rest. They were partial sales, priced far below market value. If you would like to inspect the valuations, follow the directions at the bottom of the codebook to access Beacon and look up houses by PID.\n",
    "\n",
    "For this assignment, we will look at two ways to deal with these outliers:\n",
    "remove these outliers from the data and *winzorize* the data. \n",
    "\n",
    "### Question 3a: removing outliers\n",
    "\n",
    "Write a function `remove_outliers` that removes outliers from a data set based off a threshold value of a variable.  For example, `remove_outliers(training_data, 'Gr_Liv_Area', upper=5000)` should return a data frame with only observations that satisfy `Gr_Liv_Area` less than 5000.  \n",
    "\n",
    "Inside the function, **don't change the original dataset inside the function**, since we will need the original dataset for the next question.\n",
    "\n",
    "*Hint*: Remember that if you use `df.loc[(df[\"Column\"] > some_value)]`, the result will return all rows for which the given boolean condition is `True`.\n",
    "\n",
    "*The provided tests check that training_data was updated correctly, so that future analyses are not corrupted by a mistake. However, the provided tests do not check that you have implemented remove_outliers correctly so that it works with any data, variable, lower, and upper bound.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9186ec2ca053d0aa",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_outliers(data, variable, lower=-np.inf, upper=np.inf):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (data frame): the table to be filtered\n",
    "      variable (string): the name of the column with numerical outliers\n",
    "      lower (numeric): observations with values lower than this will be removed\n",
    "      upper (numeric): observations with values higher than this will be removed\n",
    "    \n",
    "    Output:\n",
    "      a new data frame with outliers removed\n",
    "      \n",
    "    Note: This function should not change the contents of data.\n",
    "    \"\"\"  \n",
    "\n",
    "    ## Remove outliers\n",
    "    ...\n",
    "\n",
    "    return data_no_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_cleaned = remove_outliers(training_data, 'Gr_Liv_Area', upper=5000)\n",
    "training_data_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the summary statistics for the newly created dataframe to what we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Original dataset\", training_data['Gr_Liv_Area'].describe())\n",
    "print(\"No outliers\", training_data_cleaned['Gr_Liv_Area'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3b: replace outliers (winsorize)\n",
    "\n",
    "Now, instead of removing outliers, we will replace them with another value. This process of replacing a certain number of outliers has become known as *Winsorization* / *Winsorizing the data*.\n",
    "\n",
    "> Winsorization began as a way to \"robustify\" the sample mean, which is sensitive to extreme values. To obtain the Winsorized mean, you sort the data and replace the smallest $k$ values by the ($k+1$)st smallest value. You do the same for the largest values... The mean of this new set of numbers is called the Winsorized mean. If the data are from a symmetric population, the Winsorized mean is a robust unbiased estimate of the population mean.\n",
    "\n",
    "(Source: [Winsorization: The good, the bad, and the ugly](https://blogs.sas.com/content/iml/2017/02/08/winsorization-good-bad-and-ugly.html); Retrieved May 2020)\n",
    "\n",
    "Note that Winsorization is supposed to be *symmetric*, which means if we are replacing the $k$ largest values, we also need to replace the $k$ smallest values. For example:\n",
    "If there are 10 data values, *sorted*, and $k=2$, then `D[0]` and `D[1]` are replaced by `D[2]` and the last two values (`D[8]`, `D[9]`) are replaced by `D[7]`.\n",
    "\n",
    "*Hint*: Remember that if you use `df.loc[(df[\"Column\"] > some_value), \"Column\"] = another_value`, the result will change all values in the `\"Column\"` of `df` for which the given boolean condition is `True` to `another_value`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize(data, variable, k=0):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (data frame): the table to be filtered\n",
    "      variable (string): the name of the column with numerical outliers\n",
    "      k (integer): the number of observations to replace\n",
    "    \n",
    "    Output:\n",
    "      a winsorized data frame with the k outliers (smallest and largest values) replaced.\n",
    "      \n",
    "    Note: This function should not change the contents of data.\n",
    "    \"\"\"\n",
    "    # Sort the values of the given variable to find the replacement values\n",
    "    sorted_val = np.sort(data[variable])\n",
    "    total = ...\n",
    "    lower = ...\n",
    "    upper = ...\n",
    "    \n",
    "    # Make a copy() of the data so not to change the original dataframe\n",
    "    data_copy = ...\n",
    "\n",
    "    ## Replace outliers on data_copy\n",
    "    ...\n",
    "\n",
    "    return data_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_winsorized = winsorize(training_data, 'Gr_Liv_Area', k=2)\n",
    "training_data_winsorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the summary statistics for the newly created dataframe to what we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original dataset:\", training_data['Gr_Liv_Area'].describe())\n",
    "print(\"No outliers:\", training_data_cleaned['Gr_Liv_Area'].describe())\n",
    "print(\"Winsorized:\", training_data_winsorized['Gr_Liv_Area'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3c: when / why to use each method\n",
    "\n",
    "Now that you have seen the effect of removing outliers versus winsorizing, briefly explain the advantage and the pitfalls of each method. Feel free to use the (above) linked resource about winsorization to explain the pros and cons in your own words.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c\n",
    "points: 3\n",
    "manual: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Part 2: Feature Engineering\n",
    "\n",
    "In this section we will create a new feature out of existing ones through a simple data transformation.\n",
    "\n",
    "### Bathrooms\n",
    "\n",
    "Let's create a groundbreaking new feature. Imagine that Total Bathrooms can be calculated as:\n",
    "\n",
    "$$ \\text{TotalBathrooms}=(\\text{BsmtFullBath} + \\text{FullBath}) + \\dfrac{1}{2}(\\text{BsmtHalfBath} + \\text{HalfBath})$$\n",
    "\n",
    "The actual proof is beyond the scope of this class, but we will use the result in our model.\n",
    "\n",
    "## Question 4: adding new features <a name=\"q4\"></a>\n",
    "\n",
    "Write a function `add_total_bathrooms(data)` that returns a copy of `data` with an additional column called `TotalBathrooms` computed by the formula above.  **Treat missing values as zeros** (*hint*: you can use [`df.fillna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)).  Remember that you can make use of vectorized code here; you shouldn't need any `for` statements. \n",
    "\n",
    "**Do not modify the original dataset.**\n",
    "\n",
    "*The provided tests check that you answered correctly, so that future analyses are not corrupted by a mistake.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_total_bathrooms(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (data frame): a data frame containing at least 4 numeric columns \n",
    "            Bsmt_Full_Bath, Full_Bath, Bsmt_Half_Bath, and Half_Bath\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "training_data = add_total_bathrooms(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 5a <a name=\"q5\"></a>\n",
    "\n",
    "Create a visualization that clearly and succintly shows that `TotalBathrooms` is associated with `SalePrice`. Avoid using `mark_point` since overplotting will be a problem. (*Hint*: Look into drawing a boxplot.)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5a\n",
    "points: 2\n",
    "manual: True\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a boxplot of sale price vs TotalBathrooms\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 5b\n",
    "\n",
    "How does the mean sale price change with `TotalBathrooms`? How does the variance for the sale price change with `TotalBathrooms`?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5b\n",
    "points: 1\n",
    "manual: True\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ffdfab3f8801658",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 3: Modeling\n",
    "\n",
    "We've reached the point where we can specify a model. But first, we will load a fresh copy of the data, just in case our code above produced any undesired side-effects. Run the cell below to store a fresh copy of the data from `ames_train.csv` in a dataframe named `full_data`. We will also store the number of rows in `full_data` in the variable `full_data_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a fresh copy of the data and get its length\n",
    "full_data = pd.read_csv(\"ames_train.csv\")\n",
    "test_data = pd.read_csv(\"ames_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 6 <a name=\"q6\"></a>\n",
    "\n",
    "In the following parts, we will work with only a sample of the full data.  Complete the function `generate_training_sample` to select a random sample of the full training dataset.\n",
    "\n",
    "To do this, first create a NumPy array named `train_indices`. `train_indices` should contain a *random* set of indices in `full_data`. By default `frac_training = 0.8` means that we will use a random 80% sample of the population.  Use `train_indices` to index into `data` to create your final `train` data frame.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-700027ec3c0adc57",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This makes the train-test split in this section reproducible across different runs \n",
    "# of the notebook. You do not need this line to run train_test_split in general\n",
    "np.random.seed(1337)\n",
    "\n",
    "def generate_training_sample(data, frac_training=0.8, replace=False):\n",
    "\n",
    "    full_data_len = len(data)\n",
    "    train_indices = np.random.choice(full_data_len, np.int(frac_training*full_data_len), replace=replace)\n",
    "    \n",
    "    # Create train by indexing into `full_data` using `train_indices`\n",
    "    train = ...\n",
    "    return train\n",
    "    \n",
    "    \n",
    "train = generate_training_sample(full_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-acdc861fd11912e9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Reusable Pipeline\n",
    "\n",
    "Throughout this assignment, you should notice that your data flows through a single processing pipeline several times.  From a software engineering perspective, it's best to define functions/methods that can apply the pipeline to any dataset.  We will now encapsulate our entire pipeline into a single function `process_data_gm`.  gm is shorthand for \"guided model\". We select a handful of features to use from the many that are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2fe1d82b2c19d1fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def select_columns(data, *columns):\n",
    "    \"\"\"Select only columns passed as arguments and valid columns.\"\"\"\n",
    "    cols = [c for c in columns if c in data.columns]\n",
    "    return data.loc[:, cols]\n",
    "\n",
    "def process_data_gm(data):\n",
    "    \"\"\"Process the data for a guided model.\"\"\"\n",
    "    data = remove_outliers(data, 'Gr_Liv_Area', upper=5000)\n",
    "    \n",
    "    # Transform Data, Select Features\n",
    "    data = add_total_bathrooms(data)\n",
    "    data = select_columns(data, \n",
    "                          'SalePrice', \n",
    "                          'Gr_Liv_Area', \n",
    "                          'Garage_Area',\n",
    "                          'TotalBathrooms',\n",
    "                         )\n",
    "    \n",
    "    # Return predictors and response variables separately\n",
    "    if 'SalePrice' in data.columns:\n",
    "        X = data.drop(['SalePrice'], axis = 1) \n",
    "        y = data.loc[:, 'SalePrice']\n",
    "    else:\n",
    "        # Return none for y if SalePrice is not known\n",
    "        X = data\n",
    "        y = None\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use `process_data_gm` to clean our data, select features, and add our `TotalBathrooms` feature all in one step! This function also splits our data into `X`, a matrix of features, and `y`, a vector of sale prices. \n",
    "\n",
    "Run the cell below to feed our training and test data through the pipeline, generating `X_train`, `y_train`, and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process our training and test data in exactly the same way\n",
    "# Our functions make this very easy!\n",
    "X_train, y_train = process_data_gm(full_data)\n",
    "\n",
    "# Note that there is no y_test in the test_data\n",
    "X_test, _ = process_data_gm(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-41994ca25b31660e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Fitting Our First Model\n",
    "\n",
    "We are finally going to fit a model!  The model we will fit can be written as follows:\n",
    "\n",
    "$$\\text{SalePrice} = \\alpha + \\beta_1 \\cdot \\text{Gr_Liv_Area} + \\beta_2 \\cdot \\text{Garage_Area} + \\beta_3 \\cdot \\text{TotalBathrooms}$$\n",
    "\n",
    "In vector notation, the same equation would be written:\n",
    "\n",
    "$$y = \\vec\\beta \\cdot \\vec{x}$$\n",
    "\n",
    "where $y$ is the SalePrice, $\\vec\\beta$ is a vector of all fitted weights (coefficients), and $\\vec{x}$ contains a 1 for the bias (offset) followed by each of the feature values.\n",
    "\n",
    "**Note:** Notice that all of our variables are continuous, except for `TotalBathrooms`, which takes on discrete ordered values (0, 0.5, 1, 1.5, ...). In this homework, we'll treat `TotalBathrooms` as a continuous quantitative variable in our model, but this might not be the best choice. We may revisit the issue in the next assignment.\n",
    "\n",
    "## Question 7a <a name=\"q7a\"></a>\n",
    "\n",
    "We will use a [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) object as our linear model. In the cell below, create a `LinearRegression` object and name it `linear_model`.\n",
    "\n",
    "**Hint:** See the `fit_intercept` parameter and make sure it is set appropriately. The intercept of our model corresponds to $\\alpha$ in the equation above.\n",
    "\n",
    "*The provided tests check that you answered correctly, so that future analyses are not corrupted by a mistake.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7a\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm\n",
    "\n",
    "linear_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 7b <a name=\"q7b\"></a>\n",
    "\n",
    "Now, remove the commenting and fill in the ellipses `...` below with `X_train`, `y_train`, and `X_test` by generating a training sample, fitting (or \"training\") the linear model and predicting sale price on the test data.\n",
    "\n",
    "With the ellipses filled in correctly, the code below should fit our linear model to the training data and generate the predicted sale prices for both the training and test datasets.\n",
    "\n",
    "*The provided tests check that you answered correctly, so that future analyses are not corrupted by a mistake.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7b\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1be99eea86f6cf57",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Fit the linear model\n",
    "...\n",
    "\n",
    "## Generate predictions\n",
    "y_fitted = ...\n",
    "y_predicted = ...\n",
    "\n",
    "## Coefficients in the linear regression model\n",
    "coefs = ...\n",
    "print(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 7c\n",
    "\n",
    "How much does an extra half-bathroom increase the predicted price of a house *according to our model*? \n",
    "\n",
    "*Hint*: a half-bathroom is a bathroom with no shower or bathtub, and in our dataset each additional half-bathroom in a house increases `TotalBathrooms` by 0.5. Put the answer *as a coefficient* in the code below.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7c\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_increase = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 8a <a name=\"q8a\"></a>\n",
    "\n",
    "Is our linear model any good at predicting house prices? Let's measure the quality of our model by calculating the **Root-Mean-Square Error (RMSE)** between our predicted house prices and the true prices stored in `SalePrice`.\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\dfrac{\\sum_{\\text{houses in test set}}(\\text{actual price of house} - \\text{predicted price of house})^2}{\\text{# of houses in data set}}}$$\n",
    "\n",
    "You can think of the RMSE as the standard deviation of the prediction errors in our model. In the cell below, write a function named `rmse` that calculates the RMSE of a model.\n",
    "\n",
    "**Hint:** Make sure you are taking advantage of vectorized code. This question can be answered without any `for` statements.\n",
    "\n",
    "*The provided tests check that you answered correctly, so that future analyses are not corrupted by a mistake.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q8a\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96600fa98a6c2e97",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def rmse(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculates RMSE from actual and predicted values\n",
    "    Input:\n",
    "      actual (1D array): vector of actual values\n",
    "      predicted (1D array): vector of predicted/fitted values\n",
    "    Output:\n",
    "      a float, the root-mean square error\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "rmse(y_train, y_fitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a359da2dda38fcdd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 8b: Understanding Sources of Uncertainty\n",
    "\n",
    "Previously we worked with a sample of size 80%.  In this problem, assume that instead we only have access to a small random sample of size 20%.  The regression coefficients we infer when we fit the model depend on the actual _random_ sample that we get.  As a consequence, our predictions for sale price will also depend on the sample of houses we use to fit out linear regression model.  This is a source of uncertainty in our model.\n",
    "\n",
    "The error from only observing a sample of houses is what we call *sampling variability*, since it reflects variation due to the random sample we use to train our model.  It is also sometimes referred to as \"reducible error\", since this source of variation can be reduced by sampling more data.  \n",
    "\n",
    "In practice, we only get one sample, so in it can be difficult to gauge how much sampling variance we actual have.  In PSTAT 120B we learn that we can use modeling assumptions to quantify this variance.  A computational approach called [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_\\(statistics\\)) is also a common method for quantifying variance and is related to the simulation we will do below.  \n",
    "\n",
    "In this problem we will explore sampling variability in simulation by looking at how our predictions change when we use a different sample.  We will look at predictions for the first three houses in the test data.  Simulate sampling 100 different times and for each time, fit the regression model on the *training data*, and then predict on the *test* data.  Compile these predictions and make a box plot of the sampling variance of our predictions on each of these three houses.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q8b\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "X_test, _ = process_data_gm(test_data)\n",
    "\n",
    "# Explore how predictions vary depending on the sample we actually get \n",
    "# by simulating 100 random samples of size 20%\n",
    "for i in range(100):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    y_predicted = ...\n",
    "    predictions.append(y_predicted)\n",
    "\n",
    "# Predicted house price based on each sample\n",
    "prediction_df = pd.DataFrame(np.stack(predictions)).loc[:, 0:2]\n",
    "prediction_df.columns=[\"House A\", \"House B\", \"House C\"]\n",
    "\n",
    "\n",
    "## Make a boxplot with Altair using the predicted house prices\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 8c: Residual Plots\n",
    "\n",
    "One way of understanding the performance (and appropriateness) of a model is through a residual plot. Make a scatter plot of the true sale price versus the residual sale price (`y_train - y_fitted`) from our linear regression model. \n",
    "\n",
    "*Hint*: Create a new tidy dataframe that contains a column with the residual values corresponding to each sale price (`'SalePrice'`).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q8c\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Ideally, we would see a horizontal line of points at 0 (perfect prediction!). The next best thing would be a homogenous set of points centered at 0. However, most of this residual error in our predictions would not dissapear with more data-- this is because the linear regression model is imperfect.  Our model is probably too simple. In particular, notice that the most expensive homes are systematically more expensive than our prediction. \n",
    "\n",
    "Error that does not dissapear with larger training size is known as \"irreducible error\" and can be due to models which are too simple or a lack of the relevant predictor variables in our dataset.\n",
    "\n",
    "In practice we need to keep in mind reducible error (e.g. the sampling variability) as well as the irreducible error due to missing important features or models which aren't exactly correct (models are almost never _exactly_ correct!)\n",
    "\n",
    "## Question 8d <a name=\"q8c\"></a>\n",
    "\n",
    "Based on question 6 and 8b, do you think your predictions would improve more if you collected more data to train your linear regression model on or used a more sophisticated model?  List one way in which you could improve upon the linear regression model.\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q8d\n",
    "points: 2\n",
    "manual: True\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You are finished with this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Running Built-in Tests\n",
    "1. All tests are in `tests` directory\n",
    "1. Each python file in `tests` is a test\n",
    "1. `grader.check('testname')` runs test `'testname'`, e.g. `'q1'`\n",
    "1. `grader.check_all()` runs all visible tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run built-in checks\n",
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pdf in classic notebook (does not work in JupyterLab)\n",
    "import nb2pdf\n",
    "nb2pdf.convert('hw5.ipynb')\n",
    "\n",
    "# To generate pdf using command-line, run in terminal,\n",
    "# nb2pdf hw5.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submission Checklist\n",
    "1. Check filename is 'hw5.ipynb'\n",
    "1. Save file to confirm all changes are on disk\n",
    "1. Run *Kernel > Restart & Run All* to execute all code from top to bottom\n",
    "1. Check `grader.check_all()` output\n",
    "1. Save file again to write any new output to disk\n",
    "1. Check generated pdf that all responses are displayed correctly\n",
    "1. Submit to Gradescope"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
