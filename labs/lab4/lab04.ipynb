{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# initializing otter-grader\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Visualization\n",
    "\n",
    "In this week's lab, your answers will not be primarily graded through your code, but through your written analysis. As a Data Scientist, programming is important, but adding analysis to complement your code and explaining what your code has unveiled are essential skills. This lab does not require statistical analysis, but it will require you to have a curious and analytical mind.\n",
    "\n",
    "## The goals for this lab\n",
    "* consider ethical implications of using available datasets and develop a framework of ethical questions to examine\n",
    "* get a quick overview of the dataset using `info()` and `describe()`\n",
    "* extract subsets of the dataframe by column and by rows that match a given expression\n",
    "* learn the advantages of and how to change the type of the columns using `astype()` function\n",
    "* use `value_counts()` to get the frequencies of values in a given column\n",
    "* review working with `pd.Series` (indexing and extracting values)\n",
    "* plot different types of bar charts, configure chart properties, including using `sort`\n",
    "* synthesize the above material and engage your spirit of exploration to independently analyze the provided dataset and find additional insights.\n",
    "\n",
    "**Reminder: this lab should be done individually.** You can discuss the big-picture ideas with others but the final code and analysis should be your own. You should not share your code or answers directly with other students; please complete your own work and keep it to yourself. **You are required to read and to abide by the policies listed on The Office of Student Conduct website**: https://studentconduct.sa.ucsb.edu/academic-integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Imports\n",
    "\n",
    "As always, import the modules we will need throughout this lab. We will also suppress a warning that has no bearing over this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "\n",
    "In this week's lab, we will be exploring a dataset consisting of Kickstarter projects that have either reached their funding goals or not. To load our dataset, we will be using Pandas' standard `read_csv()` method to pull our text file into a Dataframe that we can manipulate. This dataset is found at [this link](https://www.kaggle.com/yashkantharia/kickstarter-campaigns/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ks-projects.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 0: Ethical Considerations\n",
    "\n",
    "Before we even begin analyzing these data, we need to make sure that there are no obvious ethical issues with this dataset or with our usage of it.\n",
    "\n",
    "See if you can figure out:\n",
    "\n",
    "* Who collected this dataset and why?\n",
    "* Can these usage or the analysis of this dataset cause any harm to those represented in the dataset? To others?\n",
    "* Is there a license that tells others how to use and attribute the authors?\n",
    "* Who or what is represented in the data? Is someone or something over-represented? Who or what _is not represented_ in the data?\n",
    "* Are the values precise enough to answer the question of interest?\n",
    "* Did the measurement process potentially distort the system under study?\n",
    "* Are there other potential ethical issues?\n",
    "\n",
    "Try to answer these questions by looking at the source of the dataset, the site that it is hosted on, the values that are stored in it, etc. Come back to this question after you are finished with the lab and see if there's anything else that you discovered that you can add here.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q0\n",
    "manual: true\n",
    "points: 5\n",
    "gradescope: show\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Data\n",
    "\n",
    "Before we delve into the dataset with a more analytic viewpoint, it is always a good idea to look at what the dataset contains from a birds-eye view. Besides the `head()` function used above, there are two important functions to give a summary of a dataframe.\n",
    "\n",
    "First, `info()` gives us an overview of how many columns there are, what types each column contains, and how many rows we have total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` function gives us summary statistics of numerical data within our dataframe. Notice how it only describes the columns that contain `int64` or `float64` types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1\n",
    "\n",
    "When we're looking at a dataset containing many columns, it is almost inevitable that not all columns are useful. In today's lab, we want to look at the following columns in the following order: \n",
    "1. \"currency\"\n",
    "1. \"main_category\"\n",
    "1. \"sub_category\"\n",
    "1. \"duration\"\n",
    "1. \"goal_usd\"\n",
    "1. \"country\"\n",
    "1. \"blurb_length\"\n",
    "1. \"name_length\"\n",
    "1. \"status\"\n",
    "\n",
    "Recall what you did in last week's lab and select those columns and place them into a new dataframe.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "manual: false\n",
    "points: 5\n",
    "gradescope: show\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = ...\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check('q1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a new overview over our dataframe using `info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the summary statistics of the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning our Dataframe's types\n",
    "\n",
    "One thing you might have noticed in the info for these dataframes is that some columns are of type `object`. This is an artifact of how `pandas` loads data from csv files. A pandas type `object` means that when it tries to load in the csv, pandas could not determine the type of the column, e.g., whether it was a string, category, time, or a column with mixed types. Using a specific pandas type that isn't `object` can allow us to manipulate the dataframe in more ways than before.\n",
    "\n",
    "We know by looking at the data, that these columns should be categorical variables. How can we change these columns so we can more easily manipulate and analyze them?\n",
    "\n",
    "We can cast the dataframe's columns to a categorical type using Pandas' `astype()` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.loc[:,\"main_category\"] = df_new.loc[:,\"main_category\"].astype('category')\n",
    "df_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the type of column `\"main_category\"` got changed to `\"category\"`.\n",
    "\n",
    "Categorical variables have multiple benefits:\n",
    "* We can define custom sort orders like Small < Medium < Large\n",
    "* They are more efficient to group by\n",
    "* Dataframes with categorical variables typically use less memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2\n",
    "\n",
    "Change all the other columns in `df_new` that have a type \"object\" to a categorical variable. Because usually dataframes can contain many more columns with this pandas loading artifact, it is more efficient for programmers to modify these columns using a loop.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "manual: false\n",
    "points: 5\n",
    "gradescope: show\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_change = ...\n",
    "\n",
    "for col_name in columns_to_change:\n",
    "    ...\n",
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check('q2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One additional benefit of doing these conversions is that categorical variables take up less memory than objects or strings. Notice how the size of the dataframe went from 11.8+ MB to 6.8 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Faulty Rows\n",
    "\n",
    "Most datasets contain rows with faulty data. Sometimes, we will see rows of data that contain a null value, signaling that some information is missing. Keeping these rows in our dataset can cause us to make faulty analysis. \n",
    "\n",
    "When we run `df_new.info()`, the output shows us that all our columns are \"non-null,\" meaning that there are no missing values. However, this doesn't mean that we don't have bad data in our dataset. Let's take a look at the description of numerical values again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3a\n",
    "\n",
    "Notice that the minimum value of `goal_usd` is `1.000000e-02`, which is \\$0.01. If we take a look at a random project on Kickstarter, we know that a project's goal has no value after the decimal place (i.e. full dollar amounts). Filter the dataframe so that we only have projects that have `goal_usd >= 1`. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "manual: false\n",
    "points: 5\n",
    "gradescope: show\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = ...\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Word of Caution about Cleaning Data\n",
    "\n",
    "Just because a set of rows in a dataset may seem faulty at first glance doesn't mean that they should be removed. In reality, there might be some underlying reason why you see certain values, so you should always be very careful when discarding any data. \n",
    "\n",
    "In the question 3a, we removed rows that had a funding goal of under a dollar. However, we overlooked a possible explanation for why those prices existed in our dataset.\n",
    "\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "Is there any reason why the rows we removed from out dataframe should not have been removed? Let's go back to our `df_new` dataset to retrieve the rows that we removed and explore them. Then, answer the question in the markdown cell below.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "manual: false\n",
    "points: 5\n",
    "gradescope: show\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_removed_projects = ...\n",
    "df_removed_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Why shouldn't we have removed most of those rows? What might explain why they ended up being \"faulty\"?\n",
    "\n",
    "*Hint: One of the categorical variables in the dataset might provide an explanation.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b1\n",
    "manual: true\n",
    "points: 5\n",
    "gradescope: show\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Now that our data has been cleaned to a certain extent, let's explore what other insights we can find. First, let's take a look at the different main categories represented in the dataset.\n",
    "\n",
    "To get a count of all the categories present in a column, we want to first use a `value_counts()` function, which returns a dataframe that contains the count of each category in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_category_counts = df_clean[\"main_category\"].value_counts()\n",
    "main_category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result from the `value_counts()` is conveniently returned in the descending order of the counts. The `\"music\"` category seems to have the most projects in this dataset.\n",
    "\n",
    "To visualize this result better, we will want to use a bar chart to show the counts of each category. `value_counts()` returns a `pd.Series`, so we need to convert it into a Dataframe for use in Altair. \n",
    "\n",
    "To get the list of categories that match up with the counts, we use `df.axes` to get the name of each value in the series. Because of how `pd.Series` axes are stored, we need to select the first item (at index 0) in the `Series.axes` to properly get the list of categories and then use the `pd.Series.values` to retrieve the corresponding counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_counts = pd.DataFrame({\n",
    "    \"cat\": main_category_counts.axes[0],\n",
    "    \"count\": main_category_counts.values\n",
    "})\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the resulting dataframe to create a bar chart using `mark_bar()`. Let's encode the category `\"cat\"` values as the x-axis and the corresponding counts as the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(category_counts).mark_bar().encode(\n",
    "    x = alt.X(\"cat\", axis = alt.Axis(title = \"Category\")),\n",
    "    y = alt.Y(\"count\", axis = alt.Axis(title = \"Count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that by default Altair placed the category labels in alphabetical order on the x-axis. However, this visualization of categories is not very informative, as the heights of the bars are all over the place. \n",
    "\n",
    "We can actually sort the counts right inside the visualization. To do this, we can set the `sort` parameter in either the X or Y axis to the other axis (i.e., the axis that we want to sort by). In this case, we want to sort the categories on the x-axis in _descending_ order by the value on the y-axis, so we set sort to `-y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alt.Chart(category_counts).mark_bar().encode(\n",
    "    x = alt.X(\"cat\", axis = alt.Axis(title = \"Category\"), sort='-y'),\n",
    "    y = alt.Y(\"count\", axis = alt.Axis(title = \"Count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now put the the quantitative value on the x-axis, we get a [horizontal bar chart](https://altair-viz.github.io/gallery/bar_chart_horizontal.html), which make the category labels easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(category_counts).mark_bar().encode(\n",
    "    y = alt.Y(\"cat\", axis = alt.Axis(title = \"Category\")),\n",
    "    x = alt.X(\"count\", axis = alt.Axis(title = \"Count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add the `sort` parameter to make the \"music\" category be at the top of the chart with the other categories sorted accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(category_counts).mark_bar().encode(\n",
    "    y = ...\n",
    "    x = ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Lastly, use the lecture notes in order to adjust the font of the labels, axes, and to add a meaningful title to the chart. \n",
    "\n",
    "*Hint: you'll need to modify options in the`configure_axis`, `configure_title`, and `properties`.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3viz\n",
    "manual: true\n",
    "points: 5\n",
    "gradescope: show\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE \"...\" WITH PLOTTING CODE\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 4\n",
    "\n",
    "What does this graph tell you about the projects that are asking for funding on Kickstarter? \n",
    "\n",
    "At a minimum, comment on the categories and their frequencies. Include any additional observation and analysis.\n",
    "Answer in the markdown cell below.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "manual: true\n",
    "points: 5\n",
    "gradescope: show\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 5a\n",
    "\n",
    "Now let's take a look at the main categories for projects that were successfully funded. Filter our clean dataset by rows where `status == \"successful\"`. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5a\n",
    "manual: false\n",
    "points: 5\n",
    "gradescope: show\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_success = ...\n",
    "df_success.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 5b\n",
    "\n",
    "Once you have filtered the dataset, follow the example to create a bar chart of the counts of categories of successful projects.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5b\n",
    "manual: true\n",
    "points: 5\n",
    "gradescope: show\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "success_counts = ...\n",
    "\n",
    "# REPLACE \"...\" WITH PLOTTING CODE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 5c\n",
    "\n",
    "What do you notice is different about this plot? Why do you see these differences? Provide your analysis on the differences in the markdown cell below.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5c\n",
    "manual: true\n",
    "points: 5\n",
    "gradescope: show\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 6\n",
    "\n",
    "As your final task for this lab, create your own analysis on another aspect of `df_clean`. Here are a couple ideas to get you started. Please create as many cells as needed below this one for your plots and your code. At the end, remember to replace the last markdown cell with your analysis. Points will be awarded for the _clarity_ of your analysis, not on the complexity of it. \n",
    "\n",
    "Some example questions that you might ask:\n",
    "\n",
    "* What do the spreads of the funding goals look like for different categories and why?\n",
    "* We took a look at the main categories, but what about sub categories? Does this have a role to play in the more volatile main categories?\n",
    "* What do the word counts of the blurbs and the names of each project have to do with their success?\n",
    "\n",
    "All the columns in our clean dataframe are listed below:\n",
    "* main_category - Category that the project is listed under\n",
    "* sub_category - Category within the main category that the project is listed under\n",
    "* duration - How many days is the Kickstarter open for funding\n",
    "* goal_usd - Minimum goal in USD for the project to succeed\n",
    "* country - Country that the Kickstarter is listed in\n",
    "* blurb_length - Word count of the blurb that explains the Kickstarter project\n",
    "* name_length - Word count of the name of the Kickstarter project\n",
    "* status - Whether the Kickstarter was \"successful\" or \"failed\" \n",
    "\n",
    "Before you submit your lab, go back to Question 0 and add any additional insights about ethical and principles of measurement considerations that you might have found.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6\n",
    "manual: true\n",
    "points: 10\n",
    "gradescope: show\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Analysis Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Running Built-in Tests\n",
    "1. All tests are in `tests` directory\n",
    "1. Each python file in `tests` is a test\n",
    "1. `grader.check('testname')` runs test `'testname'`, e.g. `'q1'`\n",
    "1. `grader.check_all()` runs all visible tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run built-in checks\n",
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pdf in classic notebook (does not work in JupyterLab)\n",
    "import nb2pdf\n",
    "nb2pdf.convert('lab04.ipynb')\n",
    "\n",
    "# To generate pdf using command-line, run in terminal,\n",
    "# nb2pdf lab04.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submission Checklist\n",
    "1. Check filename is 'lab04.ipynb'\n",
    "1. Save file to confirm all changes are on disk\n",
    "1. Run *Kernel > Restart & Run All* to execute all code from top to bottom\n",
    "1. Check `grader.check_all()` output\n",
    "1. Save file again to write any new output to disk\n",
    "1. Check generated pdf that all responses are displayed correctly\n",
    "1. Submit to Gradescope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
