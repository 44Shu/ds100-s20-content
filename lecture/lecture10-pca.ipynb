{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is adopted from the article [PCA Example in Python with scikit-learn](https://cmdlinetips.com/2018/03/pca-example-in-python-with-scikit-learn/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# https://cmdlinetips.com/2018/03/pca-example-in-python-with-scikit-learn/\n",
    "# load make_blobs to simulate data\n",
    "from sklearn.datasets import make_blobs\n",
    "# load decomposition to do PCA analysis with sklearn\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_blobs` is one of the modules available in `scikit-learn` to construct simulated data. `make_blobs` can be easily used to make data set with multiple gaussian clusters and is widely used to test clustering algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(make_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use `make_blobs` to generate 100 x 10 matrix data, such that there are 100 samples with 10 observations (`n_features=10`). These 100 samples were generated from four different clusters (`centers=4`). Since it is simulated, we know which cluster each sample belong to (i.e., \"cluster assignment\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X1, Y1 = make_blobs(n_features=10, \n",
    "         n_samples=100,\n",
    "         centers=4, random_state=4,\n",
    "         cluster_std=2)\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `X1` is the 100 x 10 data and `Y1` is cluster assignment for the 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "  'Y': Y1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.DataFrame(X1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns to be able to plot it in Altair\n",
    "# WARNING: running this again will rename the columns again!\n",
    "col_names = list(df.columns)\n",
    "df_renamed = df\n",
    "for col in col_names:\n",
    "    df_renamed = df_renamed.rename(columns={col : \"F\"+str(col)})\n",
    "    \n",
    "df_renamed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## alt.Chart(df).mark_circle().encode(\n",
    "##     x = \"1\",\n",
    "##     y = \"0\"\n",
    "## )\n",
    "## The code above wasn't working with the original dataframe\n",
    "## which is why we renamed the columns.\n",
    "## Now, we can plot columns and visualize them against one another\n",
    "## Try changing the column names to see how the relationship changes.\n",
    "alt.Chart(df_renamed).mark_circle().encode(\n",
    "    x = \"F3\",\n",
    "    y = \"F1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is just an alternative way to visualize the range of values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.melt('Y')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alt.Chart(df1).mark_point().encode(\n",
    "    alt.X('variable'),\n",
    "    alt.Y('value')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df1).mark_point().encode(\n",
    "    alt.X('variable'),\n",
    "    alt.Y('value'),\n",
    "    color='Y:N'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~The simulated data is already centered and scaled, so we can go ahead and fit PCA model.~~\n",
    "\n",
    "**IMPORTANT**: Read more about the [Importance of Feature Scaling](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py).\n",
    "\n",
    "\n",
    "    \"Feature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.\n",
    "\n",
    "    ... In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the ‘weight’ axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_X1 = X1 - np.mean(X1)\n",
    "norm_X1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##  \n",
    "# np.mean(df)\n",
    "# df - np.mean(df)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit PCA model using `fit_transform function` to our data X1 and the result `pc` contains the **principal components**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pca.fit_transform(X1)\n",
    "pc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: manually computing SVD\n",
    "\n",
    "PCA is a specific application of the singular value decomposition (SVD) for matrices. If we have a data matrix $X$, we can decompose it into $U$, $\\Sigma$ and $V^T$ such that $$X = U \\Sigma V^T.$$ Here, $U$ is the left singular vectors, $\\Sigma$ is a diagonal matrix containing the singular values, and $V^T$ are the right singular vectors.\n",
    "\n",
    "\n",
    "$U$ is orthonormal, so $U^T U = I$\n",
    "\n",
    "and\n",
    "\n",
    "$V^T$ is orthonormal, so $V^T V = I$.\n",
    "\n",
    "Re-writing the above equation (by right-multiplying by $V$) gives us $XV = U \\Sigma V^T V$ resulting in \n",
    "$$XV = U \\Sigma,$$ where $U \\Sigma$ is our reduced data.\n",
    "\n",
    "Our original data matrix $X$ has $n$ observations and $p$ variabes (features), so its dimensions are $n \\times p$. If we want to reduce our original $p$ features down to $r$ features ($r$ much less than $p$), our matrices need to be as follows:\n",
    "\n",
    "$X$ is $n \\times p$\n",
    "\n",
    "$U$ is $n \\times r$ (orthonormal)\n",
    "\n",
    "$\\Sigma$ is $r \\times r$ (diagonal matrix)\n",
    "\n",
    "$V^T$ is $r \\times p$ (orthonormal)\n",
    "\n",
    "In the following cell, use the [`np.linalg.svd`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) function to compute the SVD of `X1`. We will store the left singular vectors, singular values, and right singular vectors in `u`, `s`, and `vt` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_X1 = pd.DataFrame(X1).sub(pd.DataFrame(X1).mean(axis=0), axis=1)\n",
    "u, s, vt = np.linalg.svd(norm_X1, full_matrices = False)\n",
    "#print(\"new mean\", norm_X1.mean())\n",
    "print(\"X:\", X1.shape)\n",
    "print(\"u:\", u.shape)\n",
    "print(\"s:\", s.shape)\n",
    "print(\"vt:\", vt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider the relationship between the singular values `s` and the variance of our data. **The total variance** is the **sum of the variances of each column** of our data. Let's compute the variance for each column of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(np.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Total var =\", sum(np.var(X1, axis=0)))\n",
    "np.var(X1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total var =\", sum(np.var(norm_X1, axis=0)))\n",
    "np.var(norm_X1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The total variance** of the data is also equal to the **sum of the squares of the singular values divided by the number of data points**, that is:\n",
    "\n",
    "$$Var(X) = \\frac{\\sum_{i=1}^k{s_i^2}}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(X1)\n",
    "total_variance_computed_from_singular_values = np.sum(np.square(s)) / N\n",
    "total_variance_computed_from_singular_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of variance captured by the $i$th principal component is equal to ($i$th singular value)$^2 / \\sum_{i=1}^k{s_i^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_of_1st_pc = s[0]**2 / sum(s**2)\n",
    "variance_of_1st_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Aside: check what sklearn.pca produced\n",
    "\n",
    "Let us make a pandas data frame with the principal components (PCs) and add the known cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df = pd.DataFrame(data = pc, \n",
    "        columns = ['PC1', 'PC2','PC3','PC4'])\n",
    "pc_df['Cluster'] = Y1 # create a new column for clusters\n",
    "\n",
    "print(pc_df.shape)\n",
    "pc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine the variance explained by each principal component. We can see that the first two principal components explains over 70% of the variation in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the variance explained by each principal component. This is also called **Scree plot**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\n",
    "    'var':pca.explained_variance_ratio_,\n",
    "    'PC':['PC1','PC2','PC3','PC4']}\n",
    ")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df2, title=\"Variance Explained by Principal Components\").mark_bar().encode(\n",
    "    alt.X('PC'),\n",
    "    alt.Y('var')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the top two principal components and make scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df2 = pc_df.drop([\"PC3\", \"PC4\"], axis=1)\n",
    "pc_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(pc_df2).mark_point().encode(\n",
    "    alt.X('PC1:Q'),\n",
    "    alt.Y('PC2:Q'),\n",
    "    #color='Cluster:N'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can clearly see the four clusters in our data. The two principal components are able to completely separate the clusters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
