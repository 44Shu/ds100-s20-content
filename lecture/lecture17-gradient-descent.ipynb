{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from vega_datasets import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example follows the narrative shown in the [Gradient Descent, Step-by-Step](https://www.youtube.com/watch?v=sDv4f4s2SB8) video. Unlike the video, we will be using the cars dataset and computing values using Pandas. The video has really nice visualizations, so I recommend cross-referencing it for additional pointers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to select just the top N cars, which have the best MPG values. Since the video uses 3 data points, we'll select the top 3 cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = data('cars')\n",
    "top5_mpg = mpg.sort_values('Miles_per_Gallon', ascending=False)[0:5]\n",
    "top3_mpg = mpg.sort_values('Miles_per_Gallon', ascending=False)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top5_mpg = top5_mpg.reset_index()\n",
    "dataset = top3_mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[['Name', 'Miles_per_Gallon', 'Horsepower']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(dataset).mark_point().encode(\n",
    "    x = \"Horsepower\",\n",
    "    y = \"Miles_per_Gallon\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Altair to find the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chart = alt.Chart(dataset).mark_point().encode(\n",
    "    alt.X(\"Horsepower\", scale=alt.Scale(domain=(40, 70))),\n",
    "    alt.Y(\"Miles_per_Gallon\", scale=alt.Scale(domain=(40, 50)))\n",
    ")\n",
    "\n",
    "chart + chart.transform_regression('Horsepower', 'Miles_per_Gallon').mark_line(color=\"red\").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fit a line to our data, if someone tells us that their car has 58 horsepower, we can use the line to predict that they get 45 miles per gallon.\n",
    "\n",
    "Let's see how the gradient descent can fit a line to data by finding the optimal values for the *intercept* and the *slope*.\n",
    "\n",
    "$$Predicted\\ MPG = intercept + slope * horsepower$$\n",
    "or, mathematically,\n",
    "\n",
    "$$ \\hat y = \\theta_0 + \\theta_1 x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the intercept \n",
    "\n",
    "Let's first use **Gradient Descent** to find the intercept. Once we understand how it works, we'll use to solve for the intercept *and* the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's plug-in the least squares estimate for the slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mpg = dataset['Miles_per_Gallon'].mean()\n",
    "est_slope = np.mean(np.power(dataset['Miles_per_Gallon'] - mean_mpg, 2))\n",
    "print(est_slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slope = (45 - 44.4)/(58 - 48)\n",
    "print(slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a _random_ value for the intercept. This is just an initial guess that gives **Gradient Descent** something to improve on.\n",
    "\n",
    "In this case, let's use 40, but technically, any number will work.\n",
    "\n",
    "$$Predicted\\ MPG = intercept + slope * horsepower$$\n",
    "now becomes\n",
    "$$Predicted\\ MPG = 40 + slope * horsepower$$\n",
    "\n",
    "It now gives us the equation for this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_intercept = 40\n",
    "\n",
    "line_df = pd.DataFrame({\n",
    "    'HP': dataset['Horsepower'],\n",
    "    'est MPG': est_intercept + slope*dataset['Horsepower'].values\n",
    "})\n",
    "line_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chart + alt.Chart(line_df).mark_line(color=\"magenta\").encode(\n",
    "    alt.X('HP'),\n",
    "    alt.Y('est MPG')\n",
    ") + chart.transform_regression('Horsepower', 'Miles_per_Gallon').mark_line(color=\"red\").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate how well this line fits the data using the **Sum of the Squared Residuals** (our Loss function). (*Note*: statistically speaking, [errors and residuals](https://en.wikipedia.org/wiki/Errors_and_residuals) are closely related but not the same.)\n",
    "\n",
    "We can first compute the residual for the first car: that's Mazda GLC with 65 HP and the corresponding 46.6 MPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[['Name', 'Miles_per_Gallon', 'Horsepower']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the predicted MPG, i.e., the point on the line, by plugging its HP into the equation of the line $$Predicted\\ MPG = 40 + 0.06 * 65.0$$ to get the predicted value of 43.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mpg = est_intercept + slope * 65 #dataset['Horsepower'].iloc[0]\n",
    "predicted_mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual is the difference between the *observed MPG* ($y$) and the *predicted MPG* ($\\hat y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset['Miles_per_Gallon'].iloc[0] - predicted_mpg\n",
    "46.6 - predicted_mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = dataset['Miles_per_Gallon'] - (est_intercept + slope * dataset['Horsepower'])\n",
    "residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of the squared residuals (SSR)...\n",
    "\n",
    "$$SSR = \\sum (y - \\hat y)^2 = \\sum (y - (\\theta_0 + \\theta_1 x))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssr_40 = np.sum(np.power(residuals,2))\n",
    "ssr_40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the SSR for various intercepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssr = []\n",
    "\n",
    "for intercept in range(40, 46):\n",
    "    residuals = dataset['Miles_per_Gallon'] - (intercept + slope * dataset['Horsepower'])\n",
    "    ssr.append(np.sum(np.power(residuals,2)))\n",
    "    \n",
    "ssr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's turn them into a dataframe for easy plotting\n",
    "ssr_df = pd.DataFrame({\n",
    "    'intercept': range(40, 46),\n",
    "    'SSR': ssr\n",
    "})\n",
    "ssr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alt.Chart(ssr_df).mark_circle().encode(\n",
    "    alt.X('intercept:O'),\n",
    "    alt.Y('SSR')\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the different lines resulting from varying the intercept, let's create the dataframe that stores the results for different lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_df = pd.DataFrame({\n",
    "    'HP': dataset['Horsepower'],\n",
    "    'MPG, i='+ str(est_intercept): est_intercept + slope*dataset['Horsepower'].values\n",
    "})\n",
    "lines_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intercept in range(41, 46):\n",
    "    column = 'MPG, i='+ str(intercept)\n",
    "    lines_df[column] = intercept + slope*dataset['Horsepower'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chart = chart + alt.Chart(lines_df).mark_line(color=\"magenta\").encode(\n",
    "        alt.X('HP'),\n",
    "        alt.Y('MPG, i=40')\n",
    ")\n",
    "new_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intercept in range(40, 42): ### ADJUST the range values to see all lines\n",
    "    column = 'MPG, i='+ str(intercept)\n",
    "    new_chart = new_chart + alt.Chart(lines_df).mark_line(color=\"orange\").encode(\n",
    "        alt.X('HP'),\n",
    "        alt.Y(column)\n",
    "    )\n",
    "new_chart # TODO: fix the y-axis labels :-))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent identifies the optimal value by taking big steps when it is far away and smaller steps when it is close.\n",
    "\n",
    "Remember plotting the SSR for the intercept estimates? We can obtain a line for the curve that goes through those points and we can take the derivative of this function to determine the slope at any value for the **intercept**.\n",
    "\n",
    "$$SSR = \\sum_i^N (y_i - \\hat y_i)^2 = \\sum_i^N (y_i - (\\theta_0 + \\theta_1 x_i))^2$$\n",
    "\n",
    "Using the chain rule, we get:\n",
    "$$\\frac{\\partial}{\\partial \\theta_0} = \\sum_i^N -2(y_i - \\theta_0 - \\theta_1 x_i)$$\n",
    "\n",
    "*Note: if we were using Linear Least Squares, we would solve for the optimal value of the intercept by finding where the slope of the line = 0.* \n",
    "\n",
    "In contrast, Gradient Descent finds the minimum value by taking steps from an initial guess (of the intercept) until it reaches the best value. This makes Gradient Descent very useful when it is not possible to solve for where the derivative = 0, and this is why it can be used in so many different situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(ssr_df).mark_circle().encode(\n",
    "    alt.X('intercept:O'),\n",
    "    alt.Y('SSR')\n",
    ").configure_axis(\n",
    "    labelFontSize=14, # change axes label font size\n",
    "    titleFontSize=16  # change axes title font size\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt0 = -2* (dataset['Miles_per_Gallon'] - (est_intercept + slope * dataset['Horsepower']))\n",
    "dt0.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when our intercept is 40, the slope of the curve (plotting the SSR against the intercept) is -9.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The closer we get to the optimal value for the Intercept, the closer the slope of the curve gets to 0. This means that when the slope of the curve is close to 0, we should take baby steps, because we are close to the optimal value, and when the slope is far from 0, we should take big steps because we are far from the optimal value. \n",
    "However, if we take a huge step, then we would increase the SSR. So the size of the step should be related to the slope, since it tells us if we should take a baby step or a big step, but we need to make sure the step is not too big. Gradient Descent determines the **Step Size** by multiplying the slope by a small number called **The Learning Rate** ($\\alpha$).\n",
    "\n",
    "*Note: if the learning rate is too large, gradient descent might not arrive at the correct answer. In practice, a reasonable Learning Rate can be determined automatically by starting large and getting smaller with each step. In general, you should't need to worry about it too much.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "dt0_res = dt0.sum()\n",
    "step_size = dt0_res * alpha\n",
    "step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this step size, we can now calculate a new intercept. This allows us to take a closer step to the optimal value. Note how much the residuals would shrink with the new intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_intercept = est_intercept - step_size\n",
    "new_intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take another step, we go back to the derivative and plug in a new intercept (40.94). That tells us a slope of the curve (-3.76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt0 = -2* (dataset['Miles_per_Gallon'] - (new_intercept + slope * dataset['Horsepower']))\n",
    "dt0.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size1 = dt0.sum() * alpha\n",
    "step_size1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the new intercept is ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_intercept1 = new_intercept - step_size1\n",
    "new_intercept1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the SSR is getting smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using new_intercept\n",
    "residuals = dataset['Miles_per_Gallon'] - (new_intercept + slope * dataset['Horsepower'])\n",
    "np.sum(np.power(residuals,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the next intercept: new_intercept1\n",
    "residuals = dataset['Miles_per_Gallon'] - (new_intercept1 + slope * dataset['Horsepower'])\n",
    "np.sum(np.power(residuals,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that the first step was larger than the second step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First step\", round(step_size, 2))\n",
    "print(\"Next step\", round(step_size1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we continue taking steps, each step gets smaller and smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does Gradient Descent know to stop taking steps?\n",
    "Gradient Descent stops when the step size is very close to 0 (which will be when the slope is very close to 0).\n",
    "\n",
    "> Step Size = Slope * Learning Rate\n",
    "\n",
    "In practice, the minimum step size is 0.001 or smaller. \n",
    "\n",
    "However, we can also include a limit on the number of steps to take before giving up (e.g., 1000). So, even if the step size is still large, if we reach the maximum number of steps, Gradient Descent will stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's review\n",
    "\n",
    "The first thing we did is decide to use the Sum of the Squared Residuals (SSR) as the **Loss Function** to evaluate how well a line fits the data.\n",
    "\n",
    "Then, we took the derivative of the SSR, i.e., the derivative of the loss function.\n",
    "\n",
    "We picked a random value for the intercept, and **calculated the value of the derivative** using this randomly chosen intercept. This value told us (the direction and) **the size of the next step**.\n",
    "\n",
    "We then calculated the **new intercept** as the difference between the old intercept and the step size.\n",
    "\n",
    "Lastly, we plugged the new intercept into the derivative and repeated everything until Step Size was close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gradient Descent to find the slope _and_ the intercept "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have two or more derivatives of the same function, they are called **a gradient**.\n",
    "\n",
    "We can now use this gradient to descent to the lowest point of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$SSR = \\sum_i^N (y_i - \\hat y_i)^2 = \\sum_i^N (y_i - (\\theta_0 + \\theta_1 x_i))^2$$\n",
    "\n",
    "Using the chain rule, we get two gradients (one for each coefficient):\n",
    "$$\\frac{\\partial}{\\partial \\theta_0} = \\sum_i^N -2(y_i - \\theta_0 - \\theta_1 x_i)$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_1} = \\sum_i^N 2(y_i - \\theta_0 - \\theta_1 x_i) \\times (-x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try to code up the algorithm yourself from start to finish\n",
    "### Start with a fixing a slope and iteratively finding the intercept.\n",
    "### Then, write a version that will find both, the slope and the intercept."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
